{"cells":[{"cell_type":"markdown","metadata":{"id":"64inoPQzjsyC"},"source":["# Assignment: Extreme Event Detection and Classification using Convolution and CNN\n","\n","## Part 1: Data Loading, Visualization, and Labeling\n","\n","### Step 1: Load and Extract Precipitation Data\n","\n","We will start by loading the precipitation data from the NetCDF file. Then, we will plot a heatmap for the 180th day to visualize the precipitation on that day.\n","\n","```python\n","import xarray as xr\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the NetCDF file, you already have this data in the classroom\n","file_path = 'path_to_your_file/PERCDR_0.25deg_2001_2010_precipitation_data.nc'\n","dataset = xr.open_dataset(file_path)\n","\n","# Extract the precipitation data\n","precipitation = dataset['precip'].values  # Replace 'precip' with the correct variable name if different\n","print(\"Data shape:\", precipitation.shape)  # Should print the shape of the data\n","\n","# Plot the heatmap for the 180th day\n","day_180 = precipitation[179]\n","plt.figure(figsize=(8, 6))\n","plt.contourf(day_180, cmap='Blues')\n","plt.colorbar(label='Precipitation (mm)')\n","plt.title('Precipitation Heatmap for the 180th Day')\n","plt.xlabel('Longitude Index')\n","plt.ylabel('Latitude Index')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rukXpBFRjsyL"},"source":["### Step 2: Define Convolution Function\n","Next, we will provide you with a filter pattern and a threshold value. You will write the ```convolution_stride_pad``` function to apply this filter to the precipitation data.\n","\n","```python\n","# Filter pattern\n","extreme_event_filter = np.array([[1, 1, 1],\n","                                [1, -8, 1],\n","                                [1, 1, 1]])\n","\n","# Threshold value\n","extreme_threshold = 75\n","\n","# Define the custom convolution function with stride and padding (same code as you saw in class)\n","def convolve_stride_pad(image, filter_pattern, stride=1, padding=0):\n","    if padding > 0:\n","        image = np.pad(image, ((padding, padding), (padding, padding)), mode='constant')\n","\n","    rows, cols = image.shape\n","    filter_size = filter_pattern.shape[0]\n","    output_rows = (rows - filter_size) // stride + 1\n","    output_cols = (cols - filter_size) // stride + 1\n","    output = np.zeros((output_rows, output_cols))\n","\n","    for i in range(0, rows - filter_size + 1, stride):\n","        for j in range(0, cols - filter_size + 1, stride):\n","            patch = image[i:i + filter_size, j:j + filter_size]\n","            output[i // stride, j // stride] = np.sum(patch * filter_pattern)\n","    \n","    return output\n","\n","# Test the function with the 180th day data\n","conv_result = convolve_stride_pad(day_180, extreme_event_filter, stride=1, padding=1)\n","print(\"Convolved result shape:\", conv_result.shape)\n","\n","# Plot the convolved result as a heatmap\n","plt.figure(figsize=(8, 6))\n","plt.imshow(conv_result, cmap='hot', interpolation='nearest')\n","plt.colorbar(label='Convolution Result')\n","plt.title('Convolution Result Heatmap for the 180th Day')\n","plt.xlabel('Convolved Longitude Index')\n","plt.ylabel('Convolved Latitude Index')\n","\n","# Add grid lines for better visualization\n","plt.grid(True, which='both', color='grey', linestyle='-', linewidth=0.5)\n","\n","# Adjust ticks to show the grid-like structure more clearly\n","plt.xticks(np.arange(-.5, conv_result.shape[1], 1), labels=np.arange(0, conv_result.shape[1] + 1, 1))\n","plt.yticks(np.arange(-.5, conv_result.shape[0], 1), labels=np.arange(0, conv_result.shape[0] + 1, 1))\n","plt.gca().set_xticks(np.arange(-.5, conv_result.shape[1], 1), minor=True)\n","plt.gca().set_yticks(np.arange(-.5, conv_result.shape[0], 1), minor=True)\n","plt.gca().grid(which='minor', color='grey', linestyle='-', linewidth=0.5)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P2P7t4CKjsyM"},"source":["### Step 3: Label the Data and Plot Heatmap\n","We will now label each day based on the convolution result and the threshold value. Then, we will plot a heatmap showing the number of extreme events.\n","```python\n","# Function to label the data\n","def label_data(data, filter_pattern, threshold):\n","    labels = []\n","    for day in range(data.shape[0]):\n","        conv_result = convolve_stride_pad(data[day], filter_pattern, stride=1, padding=1)\n","        max_value = np.max(conv_result)\n","        label = 1 if max_value > threshold else 0\n","        labels.append(label)\n","    return np.array(labels)\n","\n","# Label the data\n","labels = label_data(precipitation, extreme_event_filter, extreme_threshold)\n","\n","# Print the number of extreme event days\n","print(\"Number of extreme event days:\", np.sum(labels))\n","\n","# Count the number of extreme events at each grid point\n","extreme_event_counts = np.zeros((precipitation.shape[1], precipitation.shape[2]))\n","\n","for day in range(precipitation.shape[0]):\n","    if labels[day] == 1:\n","        extreme_event_counts += precipitation[day] > extreme_threshold\n","\n","# Plot the heatmap of extreme events\n","plt.figure(figsize=(10, 8))\n","plt.contourf(extreme_event_counts, cmap='hot', levels=10)\n","plt.colorbar(label='Number of Extreme Events')\n","plt.xlabel('Longitude Index')\n","plt.ylabel('Latitude Index')\n","plt.title('Heatmap of Extreme Events')\n","plt.show()\n","```"]},{"cell_type":"markdown","metadata":{"id":"tG3rhZnwjsyN"},"source":["## Part 2: Training and Testing a CNN Classifier\n","### Step 4: Data Preparation\n","We will provide the details for loading and preparing the data for training and testing.\n","\n","```python\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Define a custom Dataset class for the labeled precipitation data\n","class PrecipitationDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        label = self.labels[idx]\n","        sample = sample[np.newaxis, :, :]  # Add channel dimension\n","        return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n","\n","# Split the data into training and testing sets\n","train_data = precipitation[:2922]  # First 8 years\n","train_labels = labels[:2922]\n","test_data = precipitation[2922:]  # Last 2 years\n","test_labels = labels[2922:]\n","\n","# Create DataLoader instances\n","train_dataset = PrecipitationDataset(train_data, train_labels)\n","test_dataset = PrecipitationDataset(test_data, test_labels)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","```"]},{"cell_type":"markdown","metadata":{"id":"h3HMM1gJjsyN"},"source":["### Step 5: Design the CNN Classifier\n","You will now design the ```CNNClassifier```. Here are the technical details:\n","\n","- Conv1: Input channels = 1, Output channels = 16, Kernel size = 3, Padding = 1\n","- Activation: ReLU\n","- Pooling: MaxPool2d with Kernel size = 2, Stride = 2\n","- Conv2: Input channels = 16, Output channels = 32, Kernel size = 3, Padding = 1\n","- FC1: Input features = 32 * 3 * 4, Output features = 64\n","- FC2: Input features = 64, Output features = 1\n","- Activation: Sigmoid for the output\n","\n","```python\n","# Your CNNClassifier model implementation here (Caution: The dimension and shape might create issues and irritate with error, try use squeeze and unsqueeze if necessary)"]},{"cell_type":"markdown","metadata":{"id":"EItTlIWojsyO"},"source":["### Step 6: Train, Test, and Evaluate the Model\n","Here is the full code implementation to train, test and evaluate the model.\n","\n","```python\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs = inputs\n","        labels = labels.unsqueeze(1)  # Convert labels to shape [batch_size, 1]\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n","\n","# Evaluate the model\n","model.eval()  # Set the model to evaluation mode\n","test_labels_list = []\n","pred_labels_list = []\n","\n","with torch.no_grad():  # Disable gradient computation for evaluation\n","    for inputs, labels in test_loader:\n","        inputs = inputs  # Ensure correct input shape\n","        outputs = model(inputs)\n","        predicted = (outputs > 0.5).float()  # Threshold the sigmoid output to get binary predictions\n","        test_labels_list.extend(labels.numpy())  # Store true labels\n","        pred_labels_list.extend(predicted.numpy().flatten())  # Store predicted labels\n","\n","# Convert lists to numpy arrays\n","test_labels_list = np.array(test_labels_list)\n","pred_labels_list = np.array(pred_labels_list)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(test_labels_list, pred_labels_list)\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n","\n","# Calculate and print precision, recall, and F1-score\n","precision = precision_score(test_labels_list, pred_labels_list)\n","recall = recall_score(test_labels_list, pred_labels_list)\n","f1 = f1_score(test_labels_list, pred_labels_list)\n","print(f'Precision: {precision:.2f}')\n","print(f'Recall: {recall:.2f}')\n","print(f'F1 Score: {f1:.2f}')\n","\n","# Generate confusion matrix\n","conf_matrix = confusion_matrix(test_labels_list, pred_labels_list)\n","print('Confusion Matrix:')\n","print(conf_matrix)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('Confusion Matrix')\n","plt.colorbar()\n","tick_marks = np.arange(2)\n","plt.xticks(tick_marks, ['Non-Extreme', 'Extreme'], rotation=45)\n","plt.yticks(tick_marks, ['Non-Extreme', 'Extreme'])\n","\n","# Add text annotations\n","thresh = conf_matrix.max() / 2.\n","for i, j in np.ndindex(conf_matrix.shape):\n","    plt.text(j, i, format(conf_matrix[i, j], 'd'),\n","             horizontalalignment=\"center\",\n","             color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n","\n","plt.tight_layout()\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","plt.show()\n","```"]},{"cell_type":"markdown","metadata":{"id":"mBu6sw84jsyO"},"source":["### Step 7: Analysis of this task and results\n","- ```Task 1:``` Provide detailed interpretation of the results (write it as a part of Task 2, i.e. no need to write it separately). (Hint: If accuracy is high but other metrics are low, you have explain in detail what it means.)\n","- ```Task 2:``` Write a detailed analysis about this assignment, from nature of data to the final results. Write as if you are writing a small paper for publication: Introduction (Problem statement), about the data and study area, methodology, results and discussion."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"pymc_env","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
